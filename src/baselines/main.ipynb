{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from time import sleep\n",
    "import nltk\n",
    "import numpy as np\n",
    "import argparse\n",
    "from langchain.llms import OpenAI\n",
    "from src.baselines.baseline_utils import load_jsonl\n",
    "from dotenv import load_dotenv\n",
    "from types import SimpleNamespace\n",
    "import asyncio\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'data_dir': '../../data/gsm_data',\n",
    "    'save_dir': 'models',\n",
    "    'debug': False,\n",
    "    'exp_label': 'default',\n",
    "    'task': 'pot_gsm',\n",
    "    'model': 'gpt-3.5-turbo',\n",
    "    'max_tokens': 2048,\n",
    "    'temperature': 0.0,\n",
    "}\n",
    "args['ckpt_path'] = os.path.join(args['save_dir'], args['exp_label'])\n",
    "args = SimpleNamespace(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = baseline_utils.Logger(os.path.join(args.ckpt_path, 'log.txt'))\n",
    "completed_rounds = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_generate_answer(llm, prompt_template, problem):\n",
    "    inp = prompt_template.format(context = problem['context'])\n",
    "    # print(inp)\n",
    "    success = False\n",
    "    while not success:\n",
    "      try:\n",
    "        output = await llm.agenerate([inp])\n",
    "        print(output)\n",
    "        success = True\n",
    "      except Exception as e:\n",
    "        logger.write(e)\n",
    "        logger.write(f'API server overloaded. Waiting for 30 seconds...')\n",
    "        sleep(30)\n",
    "        continue\n",
    "    problem['output'] = output.generations[0][0].text\n",
    "    global completed_rounds\n",
    "    completed_rounds += 1\n",
    "    print(f\"Completed {completed_rounds} rounds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_generate_answers(llm, prompt_template, problems):\n",
    "  '''Generate the answer for the given problem.'''\n",
    "  outputs = [async_generate_answer(llm, prompt_template, prob) for prob in problems]\n",
    "  await asyncio.gather(*outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def gsm_run(prompt_template, llm, data):\n",
    "    global completed_rounds\n",
    "    completed_rounds = 0\n",
    "    problems = [{'context': d['input'], 'target': d['target']} for d in data]\n",
    "    step = 5\n",
    "    for i in range(0, len(problems), step):\n",
    "        await async_generate_answers(llm, prompt_template, problems[i:min(i + step, len(problems))])\n",
    "        print (f\"Completed {i + step} problems\")\n",
    "    return problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(problems):\n",
    "    return sum([p['correct'] for p in problems]) / len(problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_answers(filename, task):\n",
    "    with open(filename, 'r') as f:\n",
    "        problems = json.loads(f.read())\n",
    "    for p in problems:\n",
    "        p['final_answer'] = baseline_utils.parse_answer(p['output'], task)\n",
    "        p['correct'] = p['final_answer'] == p['target']\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(json.dumps(problems) + '\\n')\n",
    "    return problems\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def gsm_baseline(model, task):\n",
    "    prompt_template = baseline_utils.create_prompt_template(task)\n",
    "    llm = OpenAI(\n",
    "        model_name=model,\n",
    "        max_tokens=args.max_tokens,\n",
    "        stop=['\\\\n\\\\n', 'A:', 'Q:'],\n",
    "        temperature=args.temperature,\n",
    "        openai_api_key = os.getenv('OPEN_AI_API_KEY')\n",
    "  ) \n",
    "    for i in range(3):\n",
    "        for variant in ['original', 'irc']:\n",
    "            data = baseline_utils.load_gsm_data(os.path.join(args.data_dir, f'gsmic_mixed_{i}_{variant}.jsonl'))\n",
    "            if (os.path.exists(os.path.join(args.save_dir, f'gsmic_mixed_{i}_{variant}_output_{model}_{task}.json')) \n",
    "                or os.path.exists(os.path.join(args.save_dir, f'hand_gsmic_mixed_{i}_{variant}_output_{model}_{task}.json'))):\n",
    "                continue\n",
    "            problems = await gsm_run(prompt_template, llm, data)\n",
    "            output_file = os.path.join(args.save_dir, f'gsmic_mixed_{i}_{variant}_output_{model}_{task}.json')\n",
    "            with open(output_file, 'w') as f:\n",
    "                  f.write(json.dumps(problems) + '\\n')\n",
    "            problems = parse_answers(output_file, task)\n",
    "\n",
    "            logger.write(f'Accuracy for gsmic_mixed_{i}_{variant} = {calc_accuracy(problems)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(args.model, args.task)\n",
    "await gsm_baseline(args.model, args.task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_question = baseline_utils.load_gsm_data(os.path.join(args.data_dir, f'gsmic_mixed_0_original.jsonl'))[2]\n",
    "\n",
    "prompt_template = baseline_utils.create_prompt_template('pot_gsm')\n",
    "llm = OpenAI(\n",
    "    model_name=args.model, \n",
    "    max_tokens=args.max_tokens, \n",
    "    stop=['\\\\n\\\\n', 'A:', 'Q:'],\n",
    "    \n",
    "    temperature=args.temperature,\n",
    "    openai_api_key = os.getenv('OPEN_AI_API_KEY')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_template.format(context = test_question['input']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tp = {**test_question, 'context': test_question['input']}\n",
    "await async_generate_answers(llm, prompt_template, [tp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(baseline_utils.parse_answer(tp['output'], 'pot_gsm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade(filepath, rewrite = False):\n",
    "    with open(filepath, 'r') as f:\n",
    "        problems = json.load(f)\n",
    "    for p in problems:\n",
    "        if (p['target'] == p['final_answer']):\n",
    "            p['correct'] = True\n",
    "    if (rewrite):\n",
    "        filepath = os.path.join('/'.join(filepath.split('/')[:-1]), 'hand_' + filepath.split('/')[-1])\n",
    "    with open(filepath, 'w') as f:\n",
    "        f.write(json.dumps(problems) + '\\n')\n",
    "    print(calc_accuracy(problems))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for task in ['0cot', '1cot_gsm', 'pot_gsm']:\n",
    "    for i in range(3):\n",
    "            for variant in ['original', 'irc']:\n",
    "                filepath = os.path.join(args.save_dir, f'hand_gsmic_mixed_{i}_{variant}_output_{args.model}_{task}.json')\n",
    "                print(filepath)\n",
    "                # parse_answers(filepath, args.task)\n",
    "                print(grade(filepath))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adversarial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
